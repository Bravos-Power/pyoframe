import importlib
import os
from snakemake.utils import available_cpu_count
from pyoframe.constants import SUPPORTED_SOLVERS

configfile: "config.yaml"

wildcard_constraints:
    problem="[a-z_]+",
    library="[a-z]+",
    solver="[a-z]+",
    size="\d+"

def generate_all_runs(problem):
    import itertools

    problem_data = config["problems"][problem]
    if problem_data is None or "size" not in problem_data:
        sizes = ["0"]
    else:
        sizes = problem_data["size"]
    
    runs = [
        (size, library, solver)
        for size, library, solver in itertools.product(
            sizes,
            config["libraries"],
            config["solvers"]
        )
    ]

    benchmarks = [
            importlib.import_module(f"benchmarks.{problem}.bm_{library.lower()}").Bench 
            if library not in ["jump"]
            else None
            for size, library, solver in runs
    ]

    runs = [
        r for r, benchmark in zip(
            runs, benchmarks
        ) if benchmark is None or benchmark.MAX_SIZE == None or r[0] <= benchmark.MAX_SIZE
    ]

    return runs



rule all:
    input:
        expand("results/{problem}/combined_results.png", problem=config["problems"])

rule plot_results:
    input:
        lambda wildcards: [
            f"results/{wildcards.problem}/{library}_{solver}_{size}.tsv"
            for size, library, solver in generate_all_runs(wildcards.problem)
        ]
    output:
        "results/{problem}/combined_results.csv",
        "results/{problem}/combined_results.png"
    script:
        "src/plot_results.py"

rule run_benchmark:
    threads: available_cpu_count()  # Force to use all cores to avoid parallel execution
    benchmark:
        repeat("results/{problem}/{library}_{solver}_{size}.tsv", config["repeat"])
    shell:
        "python src/run_benchmark.py {wildcards.problem} --library {wildcards.library} --solver {wildcards.solver} --size {wildcards.size}"



rule profile:
    threads: available_cpu_count()
    output:
        "results/{problem}/{library}_{solver}_{size}.prof"
    shell:
        "python -m cProfile -o results/{wildcards.problem}/{wildcards.library}_{wildcards.solver}_{wildcards.size}.prof src/run_benchmark.py {wildcards.problem} --library {wildcards.library} --solver {wildcards.solver} --size {wildcards.size}"

rule snakeviz:
    input:
        "results/{problem}/{library}_{solver}_{size}.prof"
    shell:
        "snakeviz results/{wildcards.problem}/{wildcards.library}_{wildcards.solver}_{wildcards.size}.prof"